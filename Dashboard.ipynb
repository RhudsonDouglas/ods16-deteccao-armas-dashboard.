{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXBdJ6q4NDKQpB5fFpujL7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RhudsonDouglas/ods16-deteccao-armas-dashboard./blob/main/Dashboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IhimILAxfGQU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JPjDi-oNeMR0"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision scikit-learn matplotlib pandas -q\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "# Métricas\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clonar o repositório\n",
        "!git clone https://github.com/poori-nuna/HOD-Benchmark-Dataset.git\n",
        "# Classe do Dataset\n",
        "class HODDataset(Dataset):\n",
        "def __init__(self, base_path, classes, transform=None, include_subsets=(\"normal\n",
        "self.base_path = base_path\n",
        "self.classes = classes\n",
        "self.transform = transform\n",
        "self.samples = []\n",
        "valid_ext = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\n",
        "for cls in classes:\n",
        "for subset in include_subsets:\n",
        "subset_path = os.path.join(base_path, cls, subset, \"jpg\")\n",
        "if os.path.exists(subset_path):\n",
        "imgs = [f for f in os.listdir(subset_path) if f.lower().endswit\n",
        "for f in imgs:\n",
        "img_path = os.path.join(subset_path, f)\n",
        "self.samples.append((img_path, cls))\n",
        "self.class_to_idx = {c: i for i, c in enumerate(sorted(classes))}\n",
        "self.idx_to_class = {i: c for c, i in self.class_to_idx.items()} # Facilita\n",
        "def __len__(self):\n",
        "return len(self.samples)\n",
        "def __getitem__(self, idx):\n",
        "img_path, cls = self.samples[idx]\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "label = self.class_to_idx[cls]\n",
        "if self.transform:\n",
        "image = self.transform(image)\n",
        "return image, label"
      ],
      "metadata": {
        "id": "onrNRhJ9eOot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para o treino, usamos Data Augmentation (cortes aleatórios, inversão) para o modelo generalizar\n",
        "melhor. Para validação, usamos apenas o resize e normalização."
      ],
      "metadata": {
        "id": "lB4hVwHTeToU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho base\n",
        "base_path = \"HOD-Benchmark-Dataset/dataset/class\"\n",
        "classes = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path\n",
        "print(f\"Classes encontradas: {classes}\")\n",
        "# Normalização padrão do ImageNet\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "std=[0.229, 0.224, 0.225])\n",
        "# Transforms de treino (com Augmentation) e validação (simples)\n",
        "data_transforms = {\n",
        "'train': transforms.Compose([\n",
        "# ALTERAÇÃO: Substituído RandomResizedCrop por Resize + RandomCrop para evi\n",
        "# Redimensiona a imagem para o tamanho desejado (128x128), garantindo o ges\n",
        "transforms.Resize((128, 128)),\n",
        "# Aplica um corte aleatório de 128x128, com padding opcional de 4 pixels.\n",
        "# Isso simula pequenas variações na posição do gesto dentro do quadro.\n",
        "transforms.RandomCrop(128, padding=4),\n",
        "# ADIÇÃO: Ajusta brilho, contraste, saturação e hue aleatoriamente.\n",
        "# ALTERAÇÃO: Adicionada probabilidade de 50% para esta transformação.\n",
        "transforms.RandomApply([\n",
        "transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hu\n",
        "], p=0.5), # Probabilidade de 50%\n",
        "# ADIÇÃO: Aplica rotação, translação, escala e cisalhamento aleatoriamente.\n",
        "# ALTERAÇÃO: Adicionada probabilidade de 40% para esta transformação.\n",
        "transforms.RandomApply([\n",
        "transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1\n",
        "], p=0.4), # Probabilidade de 40%\n",
        "transforms.RandomHorizontalFlip(), # Mantido. Por padrão, p=0.5 (50%).\n",
        "# ADIÇÃO: Aplica distorção de perspectiva aleatória.\n",
        "# ALTERAÇÃO: Adicionada probabilidade de 20% para esta transformação.\n",
        "transforms.RandomApply([\n",
        "transforms.RandomPerspective(distortion_scale=0.5)\n",
        "], p=0.2), # Probabilidade de 20%\n",
        "transforms.ToTensor(),\n",
        "normalize\n",
        "]),\n",
        "'val': transforms.Compose([\n",
        "transforms.Resize((128, 128)),\n",
        "transforms.ToTensor(),\n",
        "normalize\n",
        "]),\n",
        "}"
      ],
      "metadata": {
        "id": "hkLJwqvBeUpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualização de amostras das imagens transformadas"
      ],
      "metadata": {
        "id": "rRfoJFFgeZHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Bloco para Visualizar Imagens Transformadas ---\n",
        "# Certifique-se de que a classe HODDataset está definida e que os paths estão corre\n",
        "# Execute esta célula DEPOIS da célula onde você define 'full_dataset', 'data_trans\n",
        "def visualize_transforms(dataset, classes, num_images_per_class=2):\n",
        "\"\"\"\n",
        "Visualiza um pequeno conjunto de imagens de cada classe após a aplicação das tr\n",
        "Args:\n",
        "dataset (Dataset): O dataset de treino (Subset) com a transform de treino a\n",
        "classes (list): Lista de nomes das classes.\n",
        "num_images_per_class (int): Número de imagens a visualizar por classe.\n",
        "\"\"\"\n",
        "# Mapeamento de índice para nome da classe do dataset original\n",
        "idx_to_class = {i: cls_name for cls_name, i in dataset.dataset.class_to_idx.ite\n",
        "# Criar um dicionário para armazenar índices de imagens por classe\n",
        "# Estes índices são os índices DENTRO do subconjunto (train_dataset)\n",
        "class_indices_in_subset = {i: [] for i in range(len(classes))}\n",
        "# Iterar sobre os índices do SUBSET para obter os rótulos\n",
        "# dataset.indices contém os índices do dataset original que estão neste subset\n",
        "for subset_idx in range(len(dataset)):\n",
        "# Obtém o índice original do dataset\n",
        "original_idx = dataset.indices[subset_idx]\n",
        "# Obtém a amostra do dataset original para pegar o rótulo\n",
        "# Não aplicamos a transform aqui, só queremos o rótulo\n",
        "_, cls = dataset.dataset.samples[original_idx]\n",
        "label = dataset.dataset.class_to_idx[cls]\n",
        "if label in class_indices_in_subset:\n",
        "class_indices_in_subset[label].append(subset_idx) # Armazena o índice D\n",
        "fig, axes = plt.subplots(len(classes), num_images_per_class, figsize=(num_image\n",
        "fig.suptitle('Imagens após Transforms de Treino', fontsize=16, y=1.02)\n",
        "for class_idx, cls_name in enumerate(classes):\n",
        "# Pega índices aleatórios para a classe atual DENTRO do subset\n",
        "# Garantindo que não pegamos mais imagens do que existem\n",
        "if not class_indices_in_subset[class_idx]: # Skip if no images for this cla\n",
        "print(f\"Warning: No images found for class '{cls_name}' in the training\n",
        "https://colab.research.google.com/drive/1wJ3sZjeSiRsnm-F8ECHnJm6Sg2KWbIy9?usp=sharing#scrollTo=oJ9eCfE7wRWI&printM…\n",
        "3/905/11/2025, 11:21\n",
        "Projetos 5 - Etapa 2 (Treinamento) - Colab\n",
        "continue\n",
        "selected_subset_indices = np.random.choice(\n",
        "class_indices_in_subset[class_idx],\n",
        "min(num_images_per_class, len(class_indices_in_subset[class_idx])),\n",
        "replace=False\n",
        ")\n",
        "for i, subset_idx in enumerate(selected_subset_indices):\n",
        "# Obtém a imagem e o label (já transformada) usando o índice DO SUBSET\n",
        "image_tensor, label_idx = dataset[subset_idx]\n",
        "# Desnormaliza a imagem para exibição (reverte a normalização do ImageN\n",
        "# Valores padrão do ImageNet: mean=[0.485, 0.456, 0.406], std=[0.229, 0\n",
        "# O tensor está em [C, H, W], precisamos para [H, W, C] para matplotlib\n",
        "image_np = image_tensor.numpy().transpose((1, 2, 0))\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "image_np = std * image_np + mean # Desnormaliza\n",
        "image_np = np.clip(image_np, 0, 1) # Garante que os valores estejam ent\n",
        "ax = axes[class_idx, i] if len(classes) > 1 else axes[i]\n",
        "ax.imshow(image_np)\n",
        "ax.set_title(f\"{cls_name}\", fontsize=10)\n",
        "ax.axis('off')\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.98]) # Ajusta layout para o suptitle\n",
        "plt.show()\n",
        "# --- Chame a função para visualizar ---\n",
        "# Certifique-se de que 'train_dataset' já tem a transform de treino aplicada!\n",
        "print(\"Visualizando algumas imagens transformadas do conjunto de treino:\")\n",
        "visualize_transforms(train_dataset, classes, num_images_per_class=3) # Altere num_i"
      ],
      "metadata": {
        "id": "M3kAVRvXeYGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet-50 pré-treinada no ImageNet.tods os pesos foram congelados e a última camada trocada\n",
        "(fc) por uma nova, com o número de saídas igual ao número de classes do dataset"
      ],
      "metadata": {
        "id": "kkgoMZbrehBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "# Carregar ResNet-50 pré-treinada\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "# Congelar os pesos da rede (só vamos treinar a camada final por enquanto)\n",
        "for param in model.parameters():\n",
        "param.requires_grad = False\n",
        "# Substituir a camada final (fc)\n",
        "num_ftrs = model.fc.in_features\n",
        "num_classes = len(classes)\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "# Mover o modelo para a GPU\n",
        "model = model.to(device)\n",
        "print(\"Modelo ResNet-50 carregado e camada final substituída.\")\n"
      ],
      "metadata": {
        "id": "R3N8cPxfehhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "para organizar o loop de treino e validaçã"
      ],
      "metadata": {
        "id": "zeAm133FerqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss (para classificação)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Otimizador (Fase 1: treina SÓ os parâmetros da camada fc)\n",
        "optimizer_head = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=5\n",
        "history_per_iteration = {'train_loss_iter': [], 'train_acc_iter': [], 'val_loss\n",
        "history_per_epoch = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_ac\n",
        "best_acc = 0.0\n",
        "global_iteration = 0\n",
        "for epoch in range(num_epochs):\n",
        "# --- Treino ---\n",
        "model.train()\n",
        "running_loss = 0.0\n",
        "running_corrects = 0\n",
        "total_samples = 0\n",
        "train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\"\n",
        "for i, (inputs, labels) in enumerate(train_bar):\n",
        "inputs, labels = inputs.to(device), labels.to(device)\n",
        "optimizer.zero_grad()\n",
        "with torch.set_grad_enabled(True):\n",
        "outputs = model(inputs)\n",
        "_, preds = torch.max(outputs, 1)\n",
        "loss = criterion(outputs, labels)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "current_loss = loss.item() * inputs.size(0)\n",
        "current_corrects = torch.sum(preds == labels.data)\n",
        "running_loss += current_loss\n",
        "running_corrects += current_corrects\n",
        "total_samples += inputs.size(0)\n",
        "# Registrar histórico por iteração\n",
        "history_per_iteration['train_loss_iter'].append(loss.item())\n",
        "history_per_iteration['train_acc_iter'].append(current_corrects.item()\n",
        "global_iteration += 1\n",
        "train_bar.set_postfix(loss=loss.item(), acc=current_corrects.item() / i\n",
        "epoch_train_loss = running_loss / total_samples\n",
        "epoch_train_acc = running_corrects.double() / total_samples\n",
        "history_per_epoch['train_loss'].append(epoch_train_loss)\n",
        "history_per_epoch['train_acc'].append(epoch_train_acc.item())\n",
        "# --- Validação ---\n",
        "model.eval()\n",
        "running_loss = 0.0\n",
        "running_corrects = 0\n",
        "total_samples = 0\n",
        "val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Validation)\n",
        "with torch.no_grad():\n",
        "for inputs, labels in val_bar:\n",
        "inputs, labels = inputs.to(device), labels.to(device)\n",
        "outputs = model(inputs)\n",
        "_, preds = torch.max(outputs, 1)\n",
        "loss = criterion(outputs, labels)\n",
        "current_loss = loss.item() * inputs.size(0)\n",
        "current_corrects = torch.sum(preds == labels.data)\n",
        "running_loss += current_loss\n",
        "running_corrects += current_corrects\n",
        "total_samples += inputs.size(0)\n",
        "# Registrar histórico por iteração (opcional para validação, mas in\n",
        "history_per_iteration['val_loss_iter'].append(loss.item())\n",
        "history_per_iteration['val_acc_iter'].append(current_corrects.item(\n",
        "val_bar.set_postfix(loss=loss.item(), acc=current_corrects.item() /\n",
        "epoch_val_loss = running_loss / total_samples\n",
        "epoch_val_acc = running_corrects.double() / total_samples\n",
        "history_per_epoch['val_loss'].append(epoch_val_loss)\n",
        "history_per_epoch['val_acc'].append(epoch_val_acc.item())\n",
        "print(f\"\n",
        "Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_train_loss:.4f}\n",
        "# Opcional: Salvar o melhor modelo\n",
        "if epoch_val_acc > best_acc:\n",
        "best_acc = epoch_val_acc\n",
        "# torch.save(model.state_dict(), 'best_model.pth') # Descomente para sa\n",
        "print(\"Treinamento concluído!\")\n",
        "return model, history_per_epoch, history_per_iteration\n",
        ""
      ],
      "metadata": {
        "id": "BXZYFeDHeuNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "treino foi rodado por algumas épocas. Como o optimizer_head só conhece os parâmetros de\n",
        "model.fc, apenas essa camada será atualizada"
      ],
      "metadata": {
        "id": "kLQc1bgxe4Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Iniciando Treinamento (Fase 1: Só o classificador) ---\")\n",
        "num_epochs_head = 5 # 5 épocas é um bom começo\n",
        "model, history_head_epoch, history_head_iteration = train_model(\n",
        "model,\n",
        "criterion,\n",
        "optimizer_head,\n",
        "train_loader,\n",
        "test_loader, # Usando test_loader para validação durante o treino\n",
        "num_epochs_head,\n",
        "device\n",
        ")\n",
        "print(\"\\n--- Treinamento (Fase 1) Concluído ---\")"
      ],
      "metadata": {
        "id": "z3ZbKSrEe4zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a rede inteira será descongelada e um novo otimizador será criado (optimizer_all) que otimiza todos\n",
        "os parâmetros. Usamos um learning rate (lr) bem baixo para não estragar os pesos pré-treinados,\n",
        "apenas ajustá-los (fine-tuning)."
      ],
      "metadata": {
        "id": "rt4tch0pe8FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Iniciando Treinamento (Fase 2: Fine-Tuning completo) ---\")\n",
        "# 1. Descongelar a rede\n",
        "for param in model.parameters():\n",
        "param.requires_grad = True\n",
        "# 2. Novo otimizador para todos os parâmetros, com lr baixo\n",
        "optimizer_all = optim.Adam(model.parameters(), lr=1e-5) # lr = 0.00001\n",
        "num_epochs_all = 5 # Mais 5 épocas de ajuste fino\n",
        "model, history_all_epoch, history_all_iteration = train_model(\n",
        "model,\n",
        "criterion,\n",
        "optimizer_all,\n",
        "train_loader,\n",
        "test_loader, # Usando test_loader para validação durante o treino\n",
        "num_epochs_all,\n",
        "device\n",
        ")"
      ],
      "metadata": {
        "id": "UI9HSgXVe8Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para fechar a Etapa 2, rodamos o modelo no conjunto de teste e plotamos a matriz de confusão"
      ],
      "metadata": {
        "id": "vpY3tN5DfA0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Iniciando Avaliação Final ---\")\n",
        "y_pred = []\n",
        "y_true = []\n",
        "model.eval() # Modo de avaliação\n",
        "with torch.no_grad():\n",
        "for inputs, labels in tqdm(test_loader, desc=\"Gerando Predições\"):\n",
        "inputs, labels = inputs.to(device), labels.to(device)\n",
        "outputs = model(inputs)\n",
        "_, preds = torch.max(outputs, 1)\n",
        "y_pred.extend(preds.cpu().numpy())\n",
        "y_true.extend(labels.cpu().numpy())\n",
        "print(\"\\n--- Matriz de Confusão ---\")"
      ],
      "metadata": {
        "id": "K-azu6yUfBBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}